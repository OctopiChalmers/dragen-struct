\section{Case Studies} \label{sec:casestudies}

% Intuitively, adding structure information to a random generator should provide
% better results when verifying a system using random testing.
%
% For instance, by generating values that are no only syntactically but also
% semantically meaningful we would expect to reach deeper stages of our program
% pipeline other than merely input parsing/validation.
%
This section describes two case studies showing that considering additional
structural information when deriving generators can consistently produce better
testing results in terms of code coverage.
%
Instead of restricting our scope to Haskell, in this work we follow a broader
evaluation approach taken previously to compare state-of-the-art techniques to
derive random data generators based on ADT definitions
\cite{grieco2017,DBLP:conf/haskell/MistaRH18}.
%

We evaluate how including additional structural information when generating a
set of random test cases (often referred as a \emph{corpus}) affects the code
coverage obtained when testing a given target program.
%
For that, we considered two external programs which expect highly structured
inputs, namely \emph{GNU CLISP}---the GNU Common Lisp
compiler\footnote{\href{https://www.gnu.org/software/gcl/}{https://www.gnu.org/software/gcl/}},
and \emph{HTML Tidy}
\footnote{\href{http://www.html-tidy.org}{http://www.html-tidy.org}}---a well
known HTML refactoring and correction utility.
%
We remark that these applications are not written in Haskell.
%
However, there exist Haskell libraries defining ADTs encoding their input
structure, i.e., Lisp and HTML values respectively. These libraries are:
\emph{hs-zuramaru}\footnote{\href{http://hackage.haskell.org/package/zuramaru}{http://hackage.haskell.org/package/zuramaru}},
implementing an embedded Lisp intepreter for a small subset of this programming
language, and
\emph{html}\footnote{\href{http://hackage.haskell.org/package/html}{http://hackage.haskell.org/package/html}},
defining a combinator library for constructing HTML values.
%
These libraries also come with serialization functions to map Haskell values
into corresponding test case files.


We firstly compiled instrumented versions of the target programs in a way that
they also return the execution path followed in the source code every time we
run them with a given input test case.
%
This let us distinguish the amount of different execution paths that a randomly
generated corpus can trigger.
%
We then used the ADTs defined on the chosen libraries to derive random
generators using \dragen and \dragenp, including structural information
extracted from the library's codebase in the case of the latter.
%
Then, we proceeded to evaluate the code coverage triggered by independent,
randomly generated corpora of different sizes varying from 100 to 1000 test
cases each.
%
In order to remove any external bias, we derived generators optimized to follow
\emph{a uniform distribution of constructors (and pattern matchings or function
  calls in the case \dragenp), and carefully adjusted their generation sizes to
  match the average test case size in bytes}.
%
This way, any noticeable difference in the code coverage can be attributed to
the present (or lack thereof) structural information when generating the test
cases.
%
Additionally, to achieve statistical significance we repeated each experiment 30
times with independently generated sets of random test cases.

\begin{figure*}[t]
  \vspace{-5pt}%
  \centering
  \input{tikz/lisp.tex}
  \hspace{5pt}%
  \input{tikz/html.tex}
  \caption{Path coverage comparison between \dragen (\ref{exp:dragen}) and
    \dragenp (\ref{exp:dragenp}). }
  \label{fig:coverage}
  \vspace{-10pt}%
\end{figure*}
%
Fig. \ref{fig:coverage} illustrates the mean number of different execution paths
triggered for different combinations of corpus size and derivation tool,
including error bars indicating the standard error of the mean on each case.
%
We proceed to describe each case study and our findings in detail as follows.

\subsection{Branching on input data}

In this first case study we wanted to evaluate the observed code coverage
differences when considering structural information present on functions pattern
matchings.


Our chosen library encodes Lisp S-expressions essentially as lists of symbols,
represented as plain strings; and literal values like booleans or integers.
%
In order to interpret Lisp programs, this unified representation of data and
code requires this library to pattern match against common patterns like
let-bindings, if-then-else expressions and arithmetic operators among others.
%
In particular, each one of these patterns match a against special symbol of the
Lisp syntax like |"let"|, |"if"| or |"+"|; and their corresponding
sub-expressions.
%
We extracted this structural information and included it into the generation
specification of our random Lisp values---which were generated by randomly
picking from a total of 6 data constructors and 8 different pattern matchings.
%
By doing this, we obtained a code coverage improvement of approximately $4\%$
using \dragenp with respect to the one obtained with \dragen (see Figure
\ref{fig:coverage} (a)).
%
While it seems a little improvement, it is worth noticing that we are generating
highly structured inputs, i.e., Lisp programs.
%
When testing interpreters, we argue that an improvement of $4\%$ is not
negligible considering the little effort that took us to specify the pattern
matchings.
%\todo[inline, author=AM]{Shall we say anything justifing this little
%  improvement?}

\subsection{Abstract interfaces}


For our second case study, we wanted to evaluate how including structural
information coming from abstract interfaces when generating random HTML values
might improve the testing performance.


The library we used for this purpose represents HTML values very much in the
same way as we exemplify in Section \ref{sec:randomtesting}, i.e., defining a
small set of general constructions representing plain text and tags---although
this library also supports HTML tag attributes as well.
%
Then, this representation is extended with a large abstract interface consisting
of combinators representing common HTML tags and tag attributes---equivalent to
the combinators |div|, |bold| and |hr| illustrated in Section \ref{sec:sources}.


In this case study we included the structure information present on the abstract
interface of this library into the generation specification of random HTML
values, resulting in a generation process that randomly picked among 4 data
constructors and 163 abstract combinators.
%
With this large amount of additional structural information, we observed an
increase of up to $83\%$ in the code coverage obtained with \dragenp with
respect to the one observed with \dragen (see Figure \ref{fig:coverage} (b)).
%
A manual inspection of the corpora generated with each tool revealed us that, in
general terms, the test cases generated with \dragen rarely represent
syntactically correct HTML values, consisting to a large extent of random
strings within and between HTML tag delimeters (|"<"|, |">"| and |"/>"|).
%
On the other hand, test cases generated with \dragenp encode much more
interesting structural information, being mostly syntactically correct.
%
We found that, in many cases, the test cases generated with \dragenp were
parsed, analysed and reported as valid HTML values by the target application.


With these results we are confident that including the structural information
present on the user codebase improves the testing performance.
%
We consider that our approach is particularly useful when the data types
encoding the shape of our data are vague or not sufficiently structured to be
used to derive powerful random generators with the common derivation techniques
based only on ADT definitions.
